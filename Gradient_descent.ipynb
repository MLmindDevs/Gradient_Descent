{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Implementation\n",
    "\n",
    "### Author: Nick Koutroumpinis, ML|mind Software Development \n",
    "\n",
    "In this tutorial we'll implement one of the most famous optimization techniques called Gradient Descent all the way from scratch... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Imagine a dataset about <b>bikers</b> we have a feature that represents their biking activity.<br> Our goal is to <b>predict</b> the calories that they lost throughout their activity which is the second variable.. our data looks like this.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          act        cals\n",
      "0   53.426804   68.777596\n",
      "1   61.530358   62.562382\n",
      "2   47.475640   71.546632\n",
      "3   59.813208   87.230925\n",
      "4   55.142188   78.211518\n",
      "5   52.211797   79.641973\n",
      "6   39.299567   59.171489\n",
      "7   48.105042   75.331242\n",
      "8   52.550014   71.300880\n",
      "9   45.419730   55.165677\n",
      "10  54.351635   82.478847\n",
      "11  44.164049   62.008923\n",
      "12  58.168471   75.392870\n",
      "13  56.727208   81.436192\n",
      "14  48.955889   60.723602\n",
      "15  44.687196   82.892504\n",
      "16  60.297327   97.379897\n",
      "17  45.618644   48.847153\n",
      "18  38.816818   56.877213\n",
      "19  66.189817   83.878565\n",
      "20  65.416052  118.591217\n",
      "21  47.481209   57.251819\n",
      "22  41.575643   51.391744\n",
      "23  51.845187   75.380652\n",
      "24  59.370822   74.765564\n",
      "25  57.310003   95.455053\n",
      "26  63.615561   95.229366\n",
      "27  46.737619   79.052406\n",
      "28  50.556760   83.432071\n",
      "29  52.223996   63.358790\n",
      "..        ...         ...\n",
      "69  31.588117   50.392670\n",
      "70  53.660932   63.642399\n",
      "71  46.682229   72.247251\n",
      "72  43.107820   57.812513\n",
      "73  70.346076  104.257102\n",
      "74  44.492856   86.642020\n",
      "75  57.504533   91.486778\n",
      "76  36.930077   55.231661\n",
      "77  55.805733   79.550437\n",
      "78  38.954769   44.847124\n",
      "79  56.901215   80.207523\n",
      "80  56.868901   83.142750\n",
      "81  34.333125   55.723489\n",
      "82  59.049741   77.634183\n",
      "83  57.788224   99.051415\n",
      "84  54.282329   79.120646\n",
      "85  51.088720   69.588898\n",
      "86  50.282836   69.510503\n",
      "87  44.211742   73.687564\n",
      "88  38.005488   61.366905\n",
      "89  32.940480   67.170656\n",
      "90  53.691640   85.668203\n",
      "91  68.765734  114.853871\n",
      "92  46.230966   90.123572\n",
      "93  68.319361   97.919821\n",
      "94  50.030174   81.536991\n",
      "95  49.239765   72.111832\n",
      "96  50.039576   85.232007\n",
      "97  48.149859   66.224958\n",
      "98  25.128485   53.454394\n",
      "\n",
      "[99 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.csv\")\n",
    "data.columns = ['act', 'cals']\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the second part we have to do some preprocessing in order to get our feature and our label. I'll do it in the most simplistic way using the pandas library.<br>There are more simple ways than this but this is the easiest and most understandable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    feature = data['act'].values.tolist()\n",
    "    target = data['cals'].values.tolist()\n",
    "    \n",
    "    return feature, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also implement a function to get our points in a 2d array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_points(data):\n",
    "    feature, target = preprocess(data)\n",
    "    points = []\n",
    "    \n",
    "    for f, t in zip(feature, target):\n",
    "        points.append([f,t])\n",
    "        \n",
    "    return np.array(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And furthermore we are gonna visualize our points in order to get a good idea of how our data looks like in a two dimensional space.. <br> <b>Tip</b>: A good data scientist must always know how his data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a8b6da5b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_it(data):\n",
    "    feature, target = preprocess(data)\n",
    "    plt.scatter(feature, target)\n",
    "    plt.show()\n",
    "\n",
    "plot_it(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we can see some corelation in our data. So now for the part we are all here for... <br>\n",
    "Let's talk some more about <b> Gradient Descent </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get there, firstly we have to talk about some algebra. <br> Our goal is to draw a line on the points above and fit on that line as many points as we can. <br> We know from algebra that the equation of a straight line is <img src=\"https://latex.codecogs.com/gif.latex?y&space;=&space;mx&space;&plus;&space;b\" title=\"y = mx + b\" />\n",
    "\n",
    "Graphicaly a straight line is like this..<br> <br> \n",
    "<img src=\"https://www.mathsisfun.com/data/images/y-mxpb-graph.gif\" title=\"y = mx + b\" />\n",
    "\n",
    "<br>So, we can actually understand that we have to find the optimal 'm' and optimal 'b' in order to draw the best line that fits most of our points. <br>\n",
    "The process to do this is called <b> Gradient Descent </b> and it's one of the most popular optimization techniques of our time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do that we'll need an error metric.<br> We'll use the sum of squared errors or <b>SSE</b> <img src=\"http://file.scirp.org/Html/3-1680093/af56ab8f-0660-44e8-bbff-29187940442b.jpg\" title=\"SSE\" />. <br>We can define this error metric as a measure of closeness where Y is our already known values (target variable) and Y hat is our predicted (m*x + b) values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go to the part where i explain how do we get the best fit... It's easy.. We get the best fit when our cost function (error metric) is minimized to it's full potential. <img src=\"https://iamtrask.github.io/img/sgd_optimal.png\" title=\"sgd\" /> To do that we have to calculate the partial derivative of our variables with the purpose to find the optimal values for them. Which means we are searching where our cost function has the lowest result. This is the value we'll take as our m (also doing the same for b). The size of the steps we do is called the learning rate,  and we want this to be a small number so we don't lose the minima because of the big steps we are doing.\n",
    "<img src=\"http://home.agh.edu.pl/~horzyk/lectures/ai/GradientDescentOfErrorFunction.jpg\" title=\"sgd\" />\n",
    "\n",
    "Above you can think of w as our m variable. Let's code it up... \n",
    "\n",
    "<b>Hint: </b>Watch out for local minima... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error(b, m, points):\n",
    "    total_error = 0\n",
    "    for i in range(0, len(points)): # iterate through our points\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        total_error += (y - (m * x + b)) ** 2  # SSE\n",
    "    return total_error / float(len(points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the error. Now we have to code a step function because as i said before we are doing really small steps every time in order to find our minima.<br> So, let's code the step gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_gradient(b_current, m_current, points, learningRate):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))  # calculate the partial derivative\n",
    "        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current)) # calculate the partial derivative\n",
    "    new_b = b_current - (learningRate * b_gradient) # adjustments to our values since we got the new gradient\n",
    "    new_m = m_current - (learningRate * m_gradient)\n",
    "    return [new_b, new_m] # return the new values of this step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement the gradient descent runner function because we'll do this for a number of itterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    for i in range(num_iterations):\n",
    "        b, m = step_gradient(b, m, np.array(points), learning_rate)\n",
    "    return [b, m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the last part we'll implement a run function with it's only purpose to run our functional code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(data):\n",
    "    points = get_points(data)\n",
    "    learning_rate = 0.00001\n",
    "    initial_b = 0 # initial y-intercept guess\n",
    "    initial_m = 0 # initial slope guess\n",
    "    num_iterations = 1000\n",
    "    print(\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, error(initial_b, initial_m, points)))\n",
    "    print(\"Running...\")\n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "    print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, error(b, m, points)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's run this baby and see what it can do.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, m = 0, error = 5611.166153823905\n",
      "Running...\n",
      "After 1000 iterations b = 0.03630798505774645, m = 1.4809270436632145, error = 111.05814974724268\n"
     ]
    }
   ],
   "source": [
    "run(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can actually see some results out of this. <br> After 1000 itterations our error got to 111 from 5611 not too shabby right? To conclude the classic gradient descent implementation is more optimized than this. <br> The method i describe is just for learning purposes.... <br> Take your time to understand this, Thanks for your time <br> Cheers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
